{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0.post101\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tensor is 5 :\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# Scalars\n",
    "s1 = torch.tensor(5)\n",
    "print(\"the tensor is {} :\".format(s1))\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s1)\n",
    "type(s1) is torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, torch.Size([]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking its dimension and shape\n",
    "s1.ndim, s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, int)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting tensor object back to python object\n",
    "s1.item() , type(s1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor v1 : tensor([7, 7])\n",
      "Number of dimensions: 1\n",
      "Shape of the tensor: torch.Size([2]), torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# tensor\n",
    "# torch.Tensor.size() produces the same result as torch.Tensor.shape attribute\n",
    "v1 = torch.tensor([7,7])\n",
    "print(f\"The tensor v1 : {v1}\")\n",
    "print(f\"Number of dimensions: {v1.ndim}\")\n",
    "print(f\"Shape of the tensor: {v1.shape}, {v1.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor v2 : tensor([[7, 7]])\n",
      "Number of dimensions: 2\n",
      "Shape of the tensor: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "v2 = torch.tensor([[7,7]])\n",
    "print(f\"The tensor v2 : {v2}\")\n",
    "print(f\"Number of dimensions: {v2.ndim}\")\n",
    "print(f\"Shape of the tensor: {v2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor v3 : \n",
      "tensor([[7],\n",
      "        [7]])\n",
      "Number of dimensions: 2\n",
      "Shape of the tensor: torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "v3 = torch.tensor([[7],[7]])\n",
    "print(f\"The tensor v3 : \\n{v3}\")\n",
    "print(f\"Number of dimensions: {v3.ndim}\")\n",
    "print(f\"Shape of the tensor: {v3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor v4 :\n",
      "tensor([[7, 7],\n",
      "        [8, 8]])\n",
      "Number of dimensions: 2\n",
      "Shape of the tensor: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "v4 = torch.tensor([[7, 7],\n",
    "                   [8, 8]])\n",
    "print(f\"The tensor v4 :\\n{v4}\")\n",
    "print(f\"Number of dimensions: {v4.ndim}\")\n",
    "print(f\"Shape of the tensor: {v4.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor t1 : \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Number of dimensions: 2\n",
      "Shape of the tensor: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "\n",
    "print(f\"The tensor t1 : \\n{t1}\")\n",
    "print(f\"Number of dimensions: {t1.ndim}\")\n",
    "print(f\"Shape of the tensor: {t1.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor t2 :\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n",
      "Number of dimensions: 3\n",
      "Shape of the tensor: torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.tensor([[[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]]])\n",
    "\n",
    "print(f\"The tensor t2 :\\n{t2}\")\n",
    "print(f\"Number of dimensions: {t2.ndim}\")\n",
    "print(f\"Shape of the tensor: {t2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6087, 0.5233, 0.1149, 0.0368, 0.7338],\n",
       "        [0.5498, 0.5969, 0.1499, 0.2130, 0.4098],\n",
       "        [0.7339, 0.2757, 0.2997, 0.0635, 0.2213],\n",
       "        [0.1771, 0.7873, 0.9100, 0.7303, 0.7664]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)\n",
    "# Returns a tensor filled with random numbers from a uniform distribution on the interval \n",
    "# size (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n",
    "randomtensor = torch.rand(4, 5)\n",
    "randomtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9872, 0.7433, 0.1704, 0.9001])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomtensor1 = torch.rand(4)\n",
    "print(randomtensor1)\n",
    "randomtensor1.ndim, randomtensor1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([224, 224, 3]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomtensor2 = torch.rand(size=(224, 224, 3))\n",
    "randomtensor2.ndim, randomtensor2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9283,  0.9503,  0.1671, -1.2486, -0.3766, -0.4467])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)\n",
    "# Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).\n",
    "# size (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n",
    "randomtensor5 = torch.randn(6)\n",
    "randomtensor5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors of all zeros and ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zerostensor = torch.zeros(size=(3, 4))\n",
    "zerostensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onestensor = torch.ones(size=(3, 4))\n",
    "onestensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Tensor)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datatype of tensors\n",
    "onestensor.dtype, type(onestensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomtensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors of a particular range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "rangetensor = torch.arange(5)\n",
    "rangetensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangetensor1 = torch.arange(0,5,2)\n",
    "rangetensor1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors of the same size as other tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)\n",
    "# input (Tensor) – the size of input will determine size of the output tensor.\n",
    "liketensor = torch.zeros_like(input=randomtensor)\n",
    "liketensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], dtype=torch.float16)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liketensor1 = torch.zeros_like(input=randomtensor, dtype=torch.float16, device='cpu')\n",
    "liketensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liketensor2 = torch.ones_like(input=randomtensor)\n",
    "liketensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Indexing and Slicing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 1, 3],\n",
      "        [5, 4, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Manipulating a 2d tensor using indexing\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "print(a[:,[1,0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Boolean indexing\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "print(a[(a.sum(axis=1)>7),:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6])\n",
      "tensor([1, 2, 3])\n",
      "tensor(2)\n",
      "tensor([[2]])\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "# Create a 2D tensor\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "print(a[1]) # accessing the second element of the first dimension\n",
    "print(a[0]) # accessing the first element of the first dimension\n",
    "print(a[0,1]) # accessing the element of the second dimension of the first element of the first dimension\n",
    "print(a[0:1,1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17]])\n",
      "tensor([[[26, 27, 28, 29, 30, 31, 32, 33]]])\n",
      "tensor([[10, 11, 12, 13, 14, 15, 16, 17],\n",
      "        [26, 27, 28, 29, 30, 31, 32, 33]])\n",
      "tensor([[10, 11, 12, 13, 14, 15, 16, 17],\n",
      "        [26, 27, 28, 29, 30, 31, 32, 33]])\n"
     ]
    }
   ],
   "source": [
    "# create a 3 D tensor\n",
    "a = torch.tensor([[[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "                   [10, 11, 12, 13, 14, 15, 16, 17]],\n",
    "                  [[18, 19, 20, 21, 22, 23, 24, 25],\n",
    "                   [26, 27, 28, 29, 30, 31, 32, 33]]]) \n",
    "\n",
    "print(a[0])\n",
    "print(a[1:,1:])\n",
    "print(a[0:,1])\n",
    "print(a[[0,1],1]) # this produces same output as the above statement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor() always copies data. If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach()or to avoid a copy. If you have a numpy array and want to avoid a copy, use torch.as_tensor().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = torch.tensor([1.0,2.0,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/7qcpm9193z3dpfcptl5y21hw0000gn/T/ipykernel_5132/4081231999.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(ten, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(ten, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten.requires_grad_(True)\n",
    "ten.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3303113 , 0.13759222],\n",
       "       [0.30192778, 0.8054955 ],\n",
       "       [0.29949424, 0.93748183],\n",
       "       [0.23748336, 0.81422569]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparr= np.random.rand(4,2)\n",
    "nparr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1060, 0.6426],\n",
       "        [0.1599, 0.7441],\n",
       "        [0.6014, 0.2814],\n",
       "        [0.4579, 0.3772]], dtype=torch.float64)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten2 = torch.tensor(nparr)\n",
    "ten2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10599141, 0.6426084 ],\n",
       "       [0.15986856, 0.74414196],\n",
       "       [0.60142378, 0.2813948 ],\n",
       "       [0.45789426, 0.37723962]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten2[0][1] = 90\n",
    "nparr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6499145 , 0.86795959],\n",
       "       [0.95124702, 0.83439964],\n",
       "       [0.12134   , 0.31840162],\n",
       "       [0.8089194 , 0.22840108]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten2[2][1]= 25\n",
    "nparr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3303, 0.1376],\n",
       "        [0.3019, 0.8055],\n",
       "        [0.2995, 0.9375],\n",
       "        [0.2375, 0.8142]], dtype=torch.float64)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten3 =torch.as_tensor(nparr)\n",
    "ten3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten3[0][1]= 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01779952, 0.07730633],\n",
       "       [0.98869322, 0.17180187],\n",
       "       [0.66388058, 0.38763681],\n",
       "       [0.53668414, 0.0094481 ]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5.])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorr = torch.tensor([3,4,5])\n",
    "tensorr.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors datatypes and device and operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10.]), torch.float32)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defauft datatype in pytorch is float32\n",
    "tensor1 = torch.tensor([8.0, 9.0, 10.0],\n",
    "                       dtype=None,\n",
    "                       device=None,\n",
    "                       requires_grad=False)\n",
    "tensor1 , tensor1.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10.], dtype=torch.float16)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2 = tensor1.type(torch.float16)\n",
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor has dtype : torch.int64\n",
      "The tensor has dtype : torch.float16\n"
     ]
    }
   ],
   "source": [
    "# We can also use torch.Tensor.to() to change the dtype\n",
    "# torch.Tensor.to() returns the same tensor if no change has been made(correct dtype and device). Otherwise, it returns a copy of the tensor with the new specified dtype\n",
    "tensor3 = torch.tensor([4, 5 , 9])\n",
    "print(f\"The tensor has dtype : {tensor3.dtype}\")\n",
    "tensor4 = tensor3.to(torch.float16)\n",
    "print(f\"The tensor has dtype : {tensor4.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor is on : cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[264], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tensor5 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m , \u001b[38;5;241m9\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe tensor is on : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor5\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m tensor6 \u001b[38;5;241m=\u001b[39m tensor5\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe tensor is on: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor6\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# device\n",
    "# torch.Tensor.to() returns the same tensor if no change has been made(correct device and dtype). Otherwise, it moves the tensor to the new specified device and dtype\n",
    "tensor5 = torch.tensor([4, 5 , 9])\n",
    "print(f\"The tensor is on : {tensor5.device }\")\n",
    "tensor6 = tensor5.to(torch.device(\"cuda\"))\n",
    "print(f\"The tensor is on: {tensor6.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The tensor is on: cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[224], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tensor7 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m4.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The tensor is on: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor7\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m tensor8 \u001b[38;5;241m=\u001b[39m tensor7\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe tensor is on: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor8\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# torch.Tensor.cuda() or torch.Tensor.cpu() can be used  to do the same thing\n",
    "\n",
    "tensor7 = torch.tensor([3.0, 4.0, 5.0])\n",
    "print(f\" The tensor is on: {tensor7.device}\")\n",
    "tensor8 = tensor7.cuda()\n",
    "print(f\"The tensor is on: {tensor8.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: The method .to() performs an inplace operation with models. We will see more about this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 10, 11])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# addition \n",
    "mytensor = torch.tensor([4, 5 ,6])\n",
    "mytensor + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([40, 50, 60])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiplication \n",
    "mytensor * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtraction \n",
    "mytensor - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition of 2 tensors: tensor([4, 5, 6]) + tensor([7, 8, 9])\n",
      "tensor([11, 13, 15])\n"
     ]
    }
   ],
   "source": [
    "# element wise addition of 2 tensors\n",
    "mytensor2 = torch.tensor([7, 8, 9])\n",
    "print(f\"addition of 2 tensors: {mytensor} + {mytensor2}\")\n",
    "print(mytensor + mytensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 15, 16])\n",
      "tensor([20, 25, 30])\n"
     ]
    }
   ],
   "source": [
    "# pytorch inbuilt functions for manipulating tensors\n",
    "print(torch.add(mytensor, 10))\n",
    "print(torch.mul(mytensor,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "tensor(122)\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "# using pytorch inbuilt function\n",
    "print(torch.matmul(mytensor,mytensor2).dtype)\n",
    "\n",
    "# using matrix multiplication operator\n",
    "print(mytensor @ mytensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor aggregation\n",
    "##### min, max, mean, sum of the whole tensor or across different rows or columns of the tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,  40,  70, 100, 130, 160])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor4 = torch.arange(10,190,30)\n",
    "mytensor4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value in the tensor: tensor([ 10,  40,  70, 100, 130, 160]) is 10\n",
      "Minimum value in the tensor: tensor([ 10,  40,  70, 100, 130, 160]) is 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum value in the tensor: {mytensor4} is {mytensor4.min()}\")\n",
    "\n",
    "# We also have Pytorch inbuilt function to do that\n",
    "print(f\"Minimum value in the tensor: {mytensor4} is {torch.min(mytensor4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(160), tensor(160))"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor4.max(), torch.max(mytensor4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of the tensor: tensor(85.)\n",
      "mean of the tensor: tensor(85.)\n"
     ]
    }
   ],
   "source": [
    "# pytorch inbuilt function mean() and torch.Tensor.mean() method expects input of either a floating point datatype or complex datatype\n",
    "\n",
    "print( \"mean of the tensor:\", mytensor4.to(torch.float32).mean())\n",
    "print( \"mean of the tensor:\", torch.mean(mytensor4.to(torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(510), tensor(510))"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor4.sum() , torch.sum(mytensor4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of min value: tensor(0)\n",
      "Index of max value: tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# argmin() and argmax() methods\n",
    "# Return the index of the minimum and maximum value of the tensor respectively\n",
    "print(\"Index of min value:\", mytensor4.argmin())\n",
    "print(\"Index of max value:\", mytensor4.argmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose of a tensor\n",
    "mytensor3 = torch.tensor([[1, 2, 3],\n",
    "                          [4, 5, 6],\n",
    "                          [7, 8, 9],\n",
    "                          [10, 11, 12]])\n",
    "\n",
    "print(f\"Original Tensor: \\n{mytensor3}, \\nTransposed Tensor: \\n{mytensor3.T}\")\n",
    "print(f\"Shape of original tensor: {mytensor3.shape},\\nShape of transposed tensor: {mytensor3.T.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different methods to manipulate tensors:\n",
    "#### Unsqueeze, Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: tensor([ 1.0933, -0.1564, -0.8223,  2.6266, -0.6616])\n",
      "Reshaped tensor shape: torch.Size([1, 5])\n",
      "Reshaped tensor: tensor([[ 1.0933, -0.1564, -0.8223,  2.6266, -0.6616]])\n"
     ]
    }
   ],
   "source": [
    "# Adding a new dimension to a tensor\n",
    "x = torch.randn(5)\n",
    "print(\"Original Tensor:\", x)\n",
    "print(\"Reshaped tensor shape:\", x[None,:].shape)\n",
    "print(\"Reshaped tensor:\",x[None,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 1, 2])\n",
      "torch.Size([3, 1, 2])\n",
      "torch.Size([3, 2, 1])\n",
      "torch.Size([1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6]])\n",
    "\n",
    "print(x.shape)\n",
    "print(x[:,None].shape)\n",
    "print(x[:,None,:].shape)\n",
    "print(x[...,None].shape)\n",
    "print(x[None,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 1, 2])\n",
      "torch.Size([1, 3, 2])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2],\n",
       "        [ 3,  4],\n",
       "        [15,  6]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a new dimension to a tensor using torch.Tensor.unsqueeze()\n",
    "x = torch.tensor([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6]])\n",
    "\n",
    "print(x.shape)\n",
    "print(x.unsqueeze(1).shape)\n",
    "print(x.unsqueeze(0).shape)\n",
    "print(x.unsqueeze(2).shape)\n",
    "\n",
    "# The output tensor shares memory with the orginal tensor, so changing its contents will change the contents of the original tensor.\n",
    "t = x.unsqueeze(2)\n",
    "t[2,0]=15\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of tensor x: torch.Size([2, 4, 1, 3])\n",
      "Shape of squeezed tensor x : torch.Size([2, 4, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "torch.Size([2, 1, 4, 1, 3, 1])\n",
      "torch.Size([2, 1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Removing dimensions of size 1 from a tensor.\n",
    "# torch.Tensor.squeeze(dim=None), or torch.squeeze(tensor, dim=None) returns a tensor with all specified dimensions of input of size 1 removed.\n",
    "# The output tensor shares memory with the orginal tensor,so changing the contents of one will change the contents of the other.\n",
    "\n",
    "x = torch.randn(2,4,1,3)\n",
    "\n",
    "print(f\"Original shape of tensor x: {x.shape}\")\n",
    "print(f\"Shape of squeezed tensor x : {x.squeeze().shape}\")\n",
    "\n",
    "y = torch.randn(2,1,4,1,3,1)\n",
    "\n",
    "print(y.squeeze().shape)\n",
    "print(y.squeeze(4).shape) # We can also specify the dimension on which we want to perform squeeze operation\n",
    "print(y.squeeze((3,5)).shape) # tuple of ints (dimenions) can also be passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View, Reshape, Permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor x: torch.Size([2, 4, 1, 3])\n",
      "Shape of view of tensor x: torch.Size([8, 3])\n",
      "tensor([[[[ 0.2541, -0.2963, -2.7909]],\n",
      "\n",
      "         [[-1.0001,  1.7217, -1.2879]],\n",
      "\n",
      "         [[-0.0036, -0.2732,  1.2095]],\n",
      "\n",
      "         [[ 0.3788, -0.9017, -0.3504]]],\n",
      "\n",
      "\n",
      "        [[[-0.6296, -0.3684,  1.2024]],\n",
      "\n",
      "         [[ 0.2568,  0.7578, -2.0405]],\n",
      "\n",
      "         [[-0.9594,  1.4222,  1.2166]],\n",
      "\n",
      "         [[ 1.0098, -0.9031,  0.1935]]]])\n",
      "tensor([[ 0.2541, -0.2963, -2.7909],\n",
      "        [-1.0001,  1.7217, -1.2879],\n",
      "        [-0.0036, -0.2732,  1.2095],\n",
      "        [ 0.3788, -0.9017, -0.3504],\n",
      "        [-0.6296, -0.3684,  1.2024],\n",
      "        [ 0.2568,  0.7578, -2.0405],\n",
      "        [-0.9594,  1.4222,  1.2166],\n",
      "        [ 1.0098, -0.9031,  0.1935]])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor.view() returns a view of the tensor with the same number of elements and specified shape\n",
    "# The viewed tensor shares memory with the original tensor if the new view size is compatible with its original size and stride. \n",
    "# For more information, refer: https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view\n",
    "\n",
    "x = torch.randn(2,4,1,3)\n",
    "print(\"Shape of tensor x:\",x.shape)\n",
    "print(\"Shape of view of tensor x:\",x.view(8,3).shape)  #returning a view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tensor y: torch.Size([2, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of the tensor y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView of the tensor y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m8\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# If the new view size of the tensor is not compatible with its original size and stride, it doesnt return a copy.\n",
    "\n",
    "x = torch.ones(4,2)\n",
    "y = x.T\n",
    "\n",
    "print(f\"Size of the tensor y: {x.T.shape}\")\n",
    "print(f\"View of the tensor y: {y.view(8)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tensor y: torch.Size([2, 4])\n",
      "View of the tensor y: \n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]])\n",
      "Another View of the tensor y: \n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# However, in this case the new view size is compatible with original size and stride of original tensor y\n",
    "z = y.view(2,2,2)\n",
    "print(f\"Size of the tensor y: {y.shape}\")\n",
    "print(f\"View of the tensor y: \\n{y.view(2,2,2)}\")\n",
    "print(f\"Another View of the tensor y: \\n{y.view(1,2,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# It returns a copy of the tensor if the dtype is changed\n",
    "\n",
    "x = torch.ones(4,2)\n",
    "print(y)\n",
    "y[0,2]=1000\n",
    "print(x)       # changing y won't change x as y is a copy of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original tensor a : torch.Size([12])\n",
      "Shape of reshaped tensor b : torch.Size([6, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.reshape() or torch.Tensor.reshape() returns a tensor of the specified shape with same number of elements\n",
    "# It would return a view if the tensor is contiguous in memory or it has compatible strides with the new tensor. Otherwise, it returns a copy.\n",
    "# For more information, refer: https://pytorch.org/docs/stable/generated/torch.reshape.html \n",
    "\n",
    "a = torch.arange(0,120,10)\n",
    "print(\"Shape of original tensor a :\",a.shape)\n",
    "b = a.reshape(6,2,1)\n",
    "print(\"Shape of reshaped tensor b :\",b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110])\n",
      "Shape of reshaped tensor c : \n",
      " (tensor([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110]), torch.Size([12]))\n",
      "Tensor a after making changes to c : tensor([  0,  10,  20,  30, 500,  50,  60,  70,  80,  90, 100, 110])\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tensor:\",a)\n",
    "c = a.reshape(-1)   # returns same output as a.ravel()\n",
    "print(f\"Shape of reshaped tensor c : \\n {c ,c.shape}\")\n",
    "\n",
    "# The reshape operation returned a copy of tensor 'a' here\n",
    "c[4]=500\n",
    "print(\"Tensor a after making changes to c :\",a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of permuted tensor: torch.Size([5, 2, 6])\n",
      "tensor([[[ 4.5720e-01, -1.0126e+00,  1.1294e+00, -5.3137e-01, -2.9312e-01,\n",
      "          -1.5121e+00],\n",
      "         [-1.2418e+00,  5.9366e-01,  7.5422e-01, -1.3300e+00,  2.2546e-01,\n",
      "           1.1712e+00],\n",
      "         [-7.1094e-01, -1.9538e-01,  7.7954e-01,  1.4606e+00, -5.3996e-01,\n",
      "           2.0702e+00],\n",
      "         [-3.7673e-01, -1.1635e+00, -9.1467e-01, -1.3022e+00,  6.3097e-01,\n",
      "          -1.0537e+00],\n",
      "         [-9.3186e-01, -9.0685e-01, -3.9013e-01,  1.0973e+00,  8.1775e-01,\n",
      "          -1.0055e+00]],\n",
      "\n",
      "        [[-1.0397e+00, -1.3099e+00, -5.9131e-01, -4.7568e-01, -1.1170e+00,\n",
      "           4.5954e-01],\n",
      "         [-1.8662e-01,  2.6337e-01,  2.1734e-01, -6.4150e-01,  5.0000e+02,\n",
      "          -7.5831e-01],\n",
      "         [-6.9555e-01, -4.3822e-01, -3.5042e-01,  1.1488e+00,  9.6820e-02,\n",
      "          -5.0933e-02],\n",
      "         [-7.7472e-01, -6.9112e-01, -5.9988e-01,  7.8703e-01, -2.2881e+00,\n",
      "          -8.1541e-01],\n",
      "         [ 1.6293e-01, -1.1879e-02,  1.7112e+00, -8.3792e-02, -8.4152e-01,\n",
      "          -1.2866e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.permute(tensor,dims) returns a view of the original tensor input with its dimensions permuted.\n",
    "\n",
    "x = torch.randn(2,5,6)\n",
    "y = torch.permute(x,(1,0,2))\n",
    "print(f\"Shape of permuted tensor: {y.shape}\")\n",
    "y[1,1,4] = 500\n",
    "print(x)    # changing y changed x because y is a view of x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten, Ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened tensor : tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])\n",
      "Changes made in x when y is changed: \n",
      " tensor([[[200,   3,   4],\n",
      "         [  5,   6,   7]],\n",
      "\n",
      "        [[  8,   9,  10],\n",
      "         [ 11,  12,  13]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor.flatten() returns a flattened tensor.\n",
    "# It returns a view if the tensor is contiguous in memory or the new view size is compatible with size and stride of original tensor. Otherwise returns a copy\n",
    "# If parameters start_dim and end_dim are passed, it flattens only the dimensions starting from start_dim to end_dim\n",
    "\n",
    "x = torch.tensor([[[2, 3, 4],\n",
    "                   [5, 6, 7]],\n",
    "                  [[8, 9, 10],\n",
    "                  [11, 12, 13]]])\n",
    "\n",
    "y = x.flatten()   # Returned a view\n",
    "print(\"Flattened tensor :\",y)\n",
    "y[0]=200\n",
    "print(\"Changes made in x when y is changed:\\n\", x)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using parameter start_dim\n",
    "x = torch.tensor([[[2, 3, 4],\n",
    "                   [5, 6, 7]],\n",
    "                  [[8, 9, 10],\n",
    "                  [11, 12, 13]]])\n",
    "\n",
    "y = x.flatten(start_dim=1)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,4)\n",
    "b = a.T\n",
    "c = b.flatten()  # a copy is created\n",
    "c[0]=10\n",
    "print(a)  # changes are not reflected in tensor a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor dimensions: 0\n",
      "Flattened tensor dimensions: 1\n"
     ]
    }
   ],
   "source": [
    "# Flattening a zero-dimensional tensor will return a one-dimensional view.\n",
    "a = torch.tensor(4)\n",
    "print(\"Original tensor dimensions:\", a.ndim)\n",
    "b = a.flatten()\n",
    "print(\"Flattened tensor dimensions:\", b.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8524, -0.8937,  0.9879,  0.5301,  1.0332,  0.5526])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor.ravel() returns a contiguous flattened tensor\n",
    "# Returns a view if the tensor is contiguous in memory or the new view size is compatible with the size and stride of original tensor. Otherwise, returns a copy\n",
    "\n",
    "x = torch.randn(3,2)\n",
    "y = x.ravel()   # x.reshape(-1) will result in the same output\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack, hstack, vstack, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of new stacked tensor1 : torch.Size([2, 2, 3])\n",
      "Shape of new stacked tensor2 : torch.Size([2, 3, 4])\n",
      "Shape of new stacked tensor3 : torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack(tesnors, dim=0) concatenates a sequence of tensors along a new dimension. All the tensor need to be of the same size.\n",
    "# dim should be between -(n+1) to n+1, where is the no. of dimensions of the tensors\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "print(f\"Shape of new stacked tensor1 :\", torch.stack((x,x),1).shape)\n",
    "print(f\"Shape of new stacked tensor2 :\", torch.stack((x,x,x,x),2).shape)\n",
    "print(f\"Shape of new stacked tensor3 :\", torch.stack((x,x),-3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New stacked tensor : tensor([1, 2, 3, 4, 5, 6]) , Size: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# torch.hstack() stacks tensors in sequence horizontally (column wise).\n",
    "# concatenates along the first axis for 1-D tensors, and along the second axis for all other tensors.\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = torch.hstack((a,b))\n",
    "print(f\"New stacked tensor : {c} , Size: {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New stacked tensor :\n",
      "tensor([[0.9835, 0.3310, 0.3619, 0.5234, 0.9089, 0.1942],\n",
      "        [0.0959, 0.6794, 0.3940, 0.6908, 0.1097, 0.9669]]) , \n",
      "Size: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,3)\n",
    "b = torch.rand(2,3)\n",
    "c = torch.hstack((a,b))\n",
    "print(f\"New stacked tensor :\\n{c} , \\nSize: {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New stacked tensor Size: torch.Size([2, 6, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,3,4,5)\n",
    "b = torch.rand(2,3,4,5)\n",
    "c = torch.hstack((a,b))\n",
    "print(f\"New stacked tensor Size: {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New stacked tensor : \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Size: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.vstack() stacks tensors in sequence vertically (row wise).\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = torch.vstack((a,b))\n",
    "print(f\"New stacked tensor : \\n{c}\")\n",
    "print(f\"Size: {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New stacked tensor Size: torch.Size([4, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,3,4,5)\n",
    "b = torch.rand(2,3,4,5)\n",
    "c = torch.vstack((a,b))\n",
    "print(f\"New stacked tensor Size: {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of new concatenated tensor1 : torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat(tensors, dim=0) concatenates the given tensors along an existing dimension.\n",
    "# All tensors must either have the same shape (except in the concatenating dimension) or be a 1-D empty tensor with size (0,).\n",
    "# dim : dimesion over which tensors are to be concatenated\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.rand(2,3)\n",
    "print(f\"Shape of new concatenated tensor1 :\", torch.cat((x,y),0).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of new concatenated tensor2 : torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Same shape except in the concatenating dimension\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.rand(4,3)\n",
    "print(f\"Shape of new concatenated tensor2 :\", torch.cat((x,y),0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of new concatenated tensor3 : torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "a = torch.tensor([])    # Empty tensor with size 0\n",
    "print(f\"Shape of new concatenated tensor3 :\", torch.cat((x,a),0).shape)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat, Expand ,repeat_interleave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated tensor1: \n",
      "tensor([[1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3]]), shape:torch.Size([4, 6])\n",
      "Repeated tensor2: \n",
      "tensor([[[1, 2, 3],\n",
      "         [1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [1, 2, 3]]]), shape:torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor.repeat(sizes) repeats the tensor along the specified dimensions.\n",
    "# sizes : The no. of times to repeat this tensor along the specified dimensions.\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(f\"Repeated tensor1: \\n{x.repeat(4, 2)}, shape:{x.repeat(4, 2).shape}\")  # repeats the tensor x 4 times in the first dim and twice in the second dim\n",
    "print(f\"Repeated tensor2: \\n{x.repeat(4, 2, 1)}, shape:{x.repeat(4, 2, 1).shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated tensor: \n",
      "tensor([1, 1, 2, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.repeat_interleave(input,repeats,dim=None) repeats elements of a tensor\n",
    "# inputs : The input tensor\n",
    "# repeats : can be a tensor or int, it specifies the no. of repetitions for each element\n",
    "# dim : The dim along which repetition will be done. If not provided, it uses a flattened i/p array and returns a flat o/p array.\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(f\"Repeated tensor: \\n{torch.repeat_interleave(x,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated tensor: \n",
      "tensor([1, 1, 2, 2, 3, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(f\"Repeated tensor: \\n{torch.repeat_interleave(x,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated tensor: \n",
      "tensor([[1, 1, 2, 2],\n",
      "        [3, 3, 4, 4]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(f\"Repeated tensor: \\n{torch.repeat_interleave(x,2,1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated tensor: \n",
      "tensor([[1, 1, 2, 3, 3, 3],\n",
      "        [3, 3, 4, 7, 7, 7]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3],\n",
    "                 [3, 4, 7]])\n",
    "print(f\"Repeated tensor: \\n{torch.repeat_interleave(x,torch.tensor([2,1,3]),1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.Tensor.expand(sizes) returns a new view of the tensor with singleton dimensions expanded to a larger size.\n",
    "# sizes: the desired expanded size, -1 when no change in size of dimension is needed\n",
    "\n",
    "a = torch.tensor([[1, 2, 3]]) # size is 1x3\n",
    "a.expand(4,3)  # changing the singleton dimension (here it is  first dimension) to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy array to Pytorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.   2.   3.   4.   5.   6.   7.   8.   9.]\n"
     ]
    }
   ],
   "source": [
    "# torch.from_numpy() creates a torch tensor from a numpy ndarray. \n",
    "# The returned tensor shares same memory as the numpy array, if the dtype remains same as the input numpy array and is on the same device(cpu)\n",
    "\n",
    "array = np.arange(1.0, 10.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "tensor[0]= 500\n",
    "print(array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "# if we change the dtype, the changes will not be reflected back \n",
    "array = np.arange(1.0, 10.0)\n",
    "tensor = torch.from_numpy(array).to(torch.float32)\n",
    "tensor[0]= 500\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.   2.   3.   4.   5.   6.   7.   8.   9.]\n"
     ]
    }
   ],
   "source": [
    "# torch.as_tensor(data, dtype=None, device=None) converts data into a tensor, sharing data and preserving autograd history if possible.\n",
    "# the data could be a tensort, list, tuple, scalar or numpy array\n",
    "# if the data is a tensor with the requested dtype and device, it returns the same tensor with the autograd history . Otherwise, it returns a copy.\n",
    "# For tensors, same can be done with the torch.tensor() function but it always creates a copy.\n",
    "# if the data is a numpy array with the requested dtype and the same device, the output tensor shares memory with the array. Otherwise, it returns a copy of it.\n",
    "\n",
    "array = np.arange(1.0, 10.0)\n",
    "tensor = torch.as_tensor(array)\n",
    "tensor[0]= 500\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(1.0, 10.0)\n",
    "tensor = torch.as_tensor(array, dtype=torch.float32) # now this will create a copy because dtype has been changed\n",
    "tensor[0]= 500\n",
    "print(array)   # changes are not reflected back in the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch tensor to Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1000,    2,    3])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.Tensor.numpy(force=False) converts a tensor into a numpy array\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "nparray = tensor.numpy()\n",
    "nparray[0] = 1000\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[396], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m nparray \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# If force is False (the default), the conversion is performed only if the tensor is on the CPU, does not require grad, does not have its conjugate bit set, and is a dtype and layout that NumPy supports. The returned ndarray and the tensor will share their storage, so changes to the tensor will be reflected in the ndarray and vice versa.\n",
    "tensor = torch.tensor([1, 2, 3]).to(device='cuda')\n",
    "nparray = tensor.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[398], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m nparray \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3], requires_grad=True)\n",
    "nparray = tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[397], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m nparray \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mnumpy(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# If force is True this is equivalent to calling t.detach().cpu().resolve_conj().resolve_neg().numpy(). If the tensor isn’t on the CPU or the conjugate or negative bit is set, the tensor won’t share its storage with the returned ndarray. \n",
    "tensor = torch.tensor([1, 2, 3]).to(device='cuda')\n",
    "nparray = tensor.numpy(force=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3367, 0.1288, 0.2345]])\n",
      "tensor([[0.3367, 0.1288, 0.2345]])\n"
     ]
    }
   ],
   "source": [
    "# Random seed\n",
    "# Random seed or seed is a number(or vector) used to initialize a pseudorandom number generator\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "x = torch.randn([1,3])\n",
    "print(x)\n",
    "\n",
    "# if a pseudorandom number generator is reinitialized with the same seed, it will produce the same sequence of numbers\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "y = torch.randn([1,3])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU access and devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# count the number of devices\n",
    "print(torch.cpu.device_count())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the tensors to GPU\n",
    "# If you haven't defined the device while creating the tensor, you can use torch.Tensor.to() function to move it to the specified device.\n",
    "# Default device is cpu \n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0]])\n",
    "\n",
    "y = x.to(device)    # using the device agnostic code to setup the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
